{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 15:10:32 WARN Utils: Your hostname, Moatasims-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.2.1 instead (on interface bridge0)\n",
      "24/11/07 15:10:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/07 15:10:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 15:10:50 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, to_date, col, mean as F_mean, last\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StockPriceAndTweetProcessor\").getOrCreate()\n",
    "\n",
    "# Define paths for stock price and tweet data\n",
    "stockprice_path = \"/Users/moatasimfarooque/Desktop/CATIA/Latest/stockprice\"  # Replace with actual path\n",
    "stocktweet_path = \"/Users/moatasimfarooque/Desktop/CATIA/Latest/stocktweet/stocktweet.csv\"  # Replace with actual path\n",
    "output_path = \"/Users/moatasimfarooque/Desktop/CATIA/Latest/processed_stocks/\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# List of desired stock Tickers\n",
    "Tickers = [\"TSLA\", \"AAPL\", \"BA\", \"DIS\", \"AMZN\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and process stock price data\n",
    "stock_dfs = []\n",
    "for Ticker in Tickers:\n",
    "    filepath = os.path.join(stockprice_path, f\"{Ticker}.csv\")\n",
    "    if os.path.exists(filepath):\n",
    "        df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "        df = df.withColumn(\"Ticker\", lit(Ticker))\n",
    "        stock_dfs.append(df)\n",
    "\n",
    "# Combine all stock data into a single DataFrame\n",
    "if stock_dfs:\n",
    "    stockprice_df = stock_dfs[0]\n",
    "    for df in stock_dfs[1:]:\n",
    "        stockprice_df = stockprice_df.union(df)\n",
    "else:\n",
    "    raise ValueError(\"No stock price data found for selected Tickers.\")\n",
    "\n",
    "# Standardize Date format and select columns\n",
    "stockprice_df = stockprice_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "stockprice_df = stockprice_df.select(\"Date\", \"Close\", \"Volume\", \"Ticker\")\n",
    "\n",
    "# Fill missing stock prices\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"Ticker\")\n",
    "    .orderBy(\"Date\")\n",
    "    .rowsBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "stockprice_df = stockprice_df.withColumn(\n",
    "    \"Close\", last(\"Close\", ignorenulls=True).over(window_spec)\n",
    ")\n",
    "stockprice_df = stockprice_df.withColumn(\n",
    "    \"Volume\", last(\"Volume\", ignorenulls=True).over(window_spec)\n",
    ")\n",
    "median_price = stockprice_df.approxQuantile(\"Close\", [0.5], 0.01)[0]  # Median estimate\n",
    "stockprice_df = stockprice_df.na.fill({\"Close\": median_price, \"Volume\": 0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a continuous daily date range\n",
    "min_date = stockprice_df.agg(F.min(\"Date\")).collect()[0][0]\n",
    "max_date = stockprice_df.agg(F.max(\"Date\")).collect()[0][0]\n",
    "date_range_df = spark.sql(\n",
    "    f\"SELECT explode(sequence(to_date('{min_date}'), to_date('{max_date}'), interval 1 day)) as Date\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 3: Ensure each Ticker has daily values by joining with date range\n",
    "Tickers_df = spark.createDataFrame([(Ticker,) for Ticker in Tickers], [\"Ticker\"])\n",
    "full_date_Ticker_df = date_range_df.crossJoin(Tickers_df)\n",
    "\n",
    "# Join full date range with stock data\n",
    "stockprice_df = full_date_Ticker_df.join(\n",
    "    stockprice_df, on=[\"Date\", \"Ticker\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill missing values using the last known value for each Ticker\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"Ticker\")\n",
    "    .orderBy(\"Date\")\n",
    "    .rowsBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "stockprice_df = stockprice_df.withColumn(\n",
    "    \"Close\", last(\"Close\", ignorenulls=True).over(window_spec)\n",
    ")\n",
    "stockprice_df = stockprice_df.withColumn(\n",
    "    \"Volume\", last(\"Volume\", ignorenulls=True).over(window_spec)\n",
    ")\n",
    "\n",
    "# Fill any remaining missing values with defaults\n",
    "median_price = stockprice_df.approxQuantile(\"Close\", [0.5], 0.01)[0]  # Median estimate\n",
    "stockprice_df = stockprice_df.na.fill({\"Close\": median_price, \"Volume\": 0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert to Pandas to continue with sentiment processing\n",
    "final_daily_df = stockprice_df.toPandas()\n",
    "final_daily_df[\"Date\"] = pd.to_datetime(final_daily_df[\"Date\"], dayfirst=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/t8zszgx56fn2_whd95bfwqz00000gn/T/ipykernel_26484/537580458.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  stocktweet_pd[\"date\"] = pd.to_datetime(stocktweet_pd[\"date\"], dayfirst=True)\n",
      "/var/folders/qp/t8zszgx56fn2_whd95bfwqz00000gn/T/ipykernel_26484/537580458.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"sentiment\"].fillna(0, inplace=True)  # Neutral sentiment if missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed stock data with single row per date saved in 'processed_stocks' folder.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Load and process tweet data\n",
    "stocktweet = spark.read.csv(stocktweet_path, header=True, inferSchema=True)\n",
    "stocktweet = stocktweet.filter(stocktweet.stock_name.isin(Tickers))\n",
    "stocktweet_pd = stocktweet.toPandas()\n",
    "\n",
    "# Sentiment analysis setup\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# Clean and analyze tweets\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text.lower()) if isinstance(text, str) else \"\"\n",
    "\n",
    "\n",
    "stocktweet_pd[\"cleaned_tweet\"] = stocktweet_pd[\"tweet\"].apply(clean_text)\n",
    "stocktweet_pd[\"date\"] = pd.to_datetime(stocktweet_pd[\"date\"], dayfirst=True)\n",
    "stocktweet_pd[\"sentiment\"] = stocktweet_pd[\"cleaned_tweet\"].apply(\n",
    "    lambda x: sia.polarity_scores(x)[\"compound\"]\n",
    ")\n",
    "\n",
    "# Prepare tweet data for merging\n",
    "stocktweet_pd = stocktweet_pd[[\"date\", \"stock_name\", \"sentiment\"]]\n",
    "stocktweet_df = stocktweet_pd.rename(columns={\"date\": \"Date\", \"stock_name\": \"Ticker\"})\n",
    "\n",
    "# Merge with stock data\n",
    "merged_df = pd.merge(final_daily_df, stocktweet_df, on=[\"Date\", \"Ticker\"], how=\"left\")\n",
    "merged_df[\"sentiment\"].fillna(0, inplace=True)  # Neutral sentiment if missing\n",
    "\n",
    "# Convert merged data back to Spark DataFrame\n",
    "merged_df = spark.createDataFrame(merged_df)\n",
    "\n",
    "# Aggregate results\n",
    "final_aggregated_df = merged_df.groupBy(\"Date\", \"Ticker\").agg(\n",
    "    F_mean(\"Close\").alias(\"Close\"),\n",
    "    F_mean(\"Volume\").alias(\"Volume\"),\n",
    "    F_mean(\"sentiment\").alias(\"sentiment\"),\n",
    ")\n",
    "\n",
    "# Save individual CSVs for each Ticker\n",
    "for Ticker in Tickers:\n",
    "    Ticker_df = final_aggregated_df.filter(\n",
    "        final_aggregated_df.Ticker == Ticker\n",
    "    ).toPandas()\n",
    "    Ticker_df=Ticker_df.sort_values(\"Date\")\n",
    "    output_filepath = os.path.join(output_path, f\"{Ticker}_processed.csv\")\n",
    "    Ticker_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    \"Processed stock data with single row per date saved in 'processed_stocks' folder.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>92.391998</td>\n",
       "      <td>50130000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>92.391998</td>\n",
       "      <td>50130000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>94.900497</td>\n",
       "      <td>80580000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>93.748497</td>\n",
       "      <td>75288000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>93.748497</td>\n",
       "      <td>75288000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2020-12-27</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>158.634506</td>\n",
       "      <td>29038000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>164.197998</td>\n",
       "      <td>113736000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>166.100006</td>\n",
       "      <td>97458000.0</td>\n",
       "      <td>0.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>164.292496</td>\n",
       "      <td>64186000.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>162.846497</td>\n",
       "      <td>59144000.0</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date Ticker       Close       Volume  sentiment\n",
       "85  2019-12-31   AMZN   92.391998   50130000.0     0.0000\n",
       "135 2020-01-01   AMZN   92.391998   50130000.0     0.0000\n",
       "65  2020-01-02   AMZN   94.900497   80580000.0     0.0000\n",
       "90  2020-01-03   AMZN   93.748497   75288000.0     0.0000\n",
       "88  2020-01-04   AMZN   93.748497   75288000.0     0.0000\n",
       "..         ...    ...         ...          ...        ...\n",
       "217 2020-12-27   AMZN  158.634506   29038000.0     0.0000\n",
       "288 2020-12-28   AMZN  164.197998  113736000.0     0.0000\n",
       "310 2020-12-29   AMZN  166.100006   97458000.0     0.0097\n",
       "218 2020-12-30   AMZN  164.292496   64186000.0     0.0000\n",
       "241 2020-12-31   AMZN  162.846497   59144000.0     0.5719\n",
       "\n",
       "[367 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ticker_df.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
